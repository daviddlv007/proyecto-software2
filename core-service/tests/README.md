# Tests de la API GraphQL con Pytest

Tests profesionales para la API GraphQL del sistema de gestiÃ³n de supermercado usando **Pytest + GQL**.

## ğŸ“¦ InstalaciÃ³n

```bash
cd core-service/tests
./run_tests.sh  # Instala automÃ¡ticamente si es necesario
```

O manualmente:
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## ğŸš€ EjecuciÃ³n RÃ¡pida

### Con el script (recomendado)
```bash
./run_tests.sh                    # Ejecutar todos con reporte HTML
./run_tests.sh -m smoke          # Solo tests rÃ¡pidos
./run_tests.sh test_ventas.py    # Solo tests de ventas
./run_tests.sh -k crear          # Tests que contengan "crear"
```

### Manual con pytest
```bash
source venv/bin/activate
pytest -v
```

### Con reporte HTML detallado
```bash
pytest --html=report.html --self-contained-html
```

### Con cobertura de cÃ³digo
```bash
pytest --cov --cov-report=html
```

### Solo tests smoke (rÃ¡pidos)
```bash
pytest -m smoke
```

### Solo tests de integraciÃ³n
```bash
pytest -m integration
```

### Tests especÃ­ficos
```bash
pytest test_categorias.py                    # Solo categorÃ­as
pytest test_ventas.py::test_crear_venta_completa  # Un test especÃ­fico
```

### Verbose (mÃ¡s detalles)
```bash
pytest -v
```

### Ver print statements
```bash
pytest -s
```

## ğŸ“ Estructura de Archivos

```
tests/
â”œâ”€â”€ conftest.py              # ConfiguraciÃ³n global y fixtures
â”œâ”€â”€ requirements.txt         # Dependencias
â”œâ”€â”€ run_tests.sh            # â­ Script ejecutor con auto-setup
â”œâ”€â”€ test_categorias.py       # Tests de categorÃ­as
â”œâ”€â”€ test_productos.py        # Tests de productos
â”œâ”€â”€ test_clientes.py         # Tests de clientes
â”œâ”€â”€ test_usuarios.py         # Tests de usuarios
â”œâ”€â”€ test_ventas.py          # Tests de ventas (integraciÃ³n)
â”œâ”€â”€ test_complete_api.py    # Script original (legacy)
â”œâ”€â”€ README.md               # Esta documentaciÃ³n
â””â”€â”€ COMPARACION.md          # ComparaciÃ³n vs script original
```

## ğŸ·ï¸ Marks (Etiquetas)

Los tests estÃ¡n marcados con etiquetas para ejecutarlos selectivamente:

- `@pytest.mark.smoke` - Tests bÃ¡sicos y rÃ¡pidos
- `@pytest.mark.integration` - Tests de integraciÃ³n complejos
- `@pytest.mark.slow` - Tests que tardan mÃ¡s tiempo

## ğŸ”§ Fixtures Disponibles

Las fixtures se definen en `conftest.py` y se reutilizan en todos los tests:

### `gql_client`
Cliente GraphQL configurado y validado contra el schema.

```python
def test_algo(gql_client):
    query = gql("query { categorias { id } }")
    result = gql_client.execute(query)
```

### `categoria_test`
Crea una categorÃ­a de prueba y la limpia automÃ¡ticamente.

```python
def test_algo(categoria_test):
    assert categoria_test['id'] is not None
    # No necesitas limpiarla, se hace automÃ¡ticamente
```

### `producto_test`
Crea un producto de prueba vinculado a una categorÃ­a.

```python
def test_algo(producto_test, categoria_test):
    assert producto_test['categoria']['id'] == categoria_test['id']
```

### `cliente_test`
Crea un cliente de prueba.

### `usuario_test`
Crea un usuario de prueba.

## ğŸ“Š Reportes

### Reporte en Terminal
```bash
pytest -v
```

Salida:
```
test_categorias.py::test_crear_categoria PASSED                    [ 10%]
test_categorias.py::test_listar_categorias PASSED                  [ 20%]
test_categorias.py::test_actualizar_categoria PASSED               [ 30%]
...
======================== 25 passed in 5.23s ==========================
```

### Reporte HTML
```bash
pytest --html=report.html --self-contained-html
```

Genera un archivo `report.html` con:
- âœ… Tests pasados/fallidos
- â±ï¸ Tiempo de ejecuciÃ³n
- ğŸ“‹ Logs detallados
- ğŸ“Š GrÃ¡ficas

### Cobertura de CÃ³digo
```bash
pytest --cov=. --cov-report=html
```

Genera `htmlcov/index.html` con:
- % de cobertura por archivo
- LÃ­neas cubiertas/no cubiertas
- Ramas condicionales

## ğŸ¯ Ejemplos de Uso

### Test Simple
```python
def test_crear_categoria(gql_client):
    mutation = gql("""
        mutation {
          createCategoria(input: {
            nombre: "Test"
            descripcion: "Desc"
          }) {
            id
            nombre
          }
        }
    """)
    
    result = gql_client.execute(mutation)
    assert result['createCategoria']['nombre'] == "Test"
```

### Test con Fixture
```python
def test_producto_con_categoria(producto_test, categoria_test):
    # producto_test ya viene creado y vinculado a categoria_test
    assert producto_test['categoria']['id'] == categoria_test['id']
```

### Test Parametrizado
```python
@pytest.mark.parametrize("precio,stock", [
    (0.01, 1),
    (999.99, 9999),
])
def test_valores_limite(gql_client, precio, stock):
    # Se ejecuta una vez por cada par de valores
    pass
```

### Test de IntegraciÃ³n
```python
@pytest.mark.integration
def test_venta_completa(gql_client, cliente_test, producto_test):
    # Test complejo que usa mÃºltiples entidades
    pass
```

## ğŸ› Debugging

### Ver detalles de un test fallido
```bash
pytest -vv --tb=long
```

### Ejecutar solo el test que fallÃ³
```bash
pytest --lf  # last-failed
```

### Ejecutar hasta que falle
```bash
pytest -x  # stop on first failure
```

### Modo debug interactivo
```bash
pytest --pdb
```

## ğŸ”„ IntegraciÃ³n con CI/CD

### GitHub Actions
```yaml
- name: Run tests
  run: |
    cd core-service/tests
    pip install -r requirements.txt
    pytest --html=report.html --self-contained-html
    
- name: Upload test report
  uses: actions/upload-artifact@v2
  if: always()
  with:
    name: test-report
    path: core-service/tests/report.html
```

### GitLab CI
```yaml
test:
  script:
    - cd core-service/tests
    - pip install -r requirements.txt
    - pytest --junitxml=report.xml
  artifacts:
    when: always
    reports:
      junit: core-service/tests/report.xml
```

## ğŸ“ˆ ComparaciÃ³n de Rendimiento

| MÃ©trica | Script Original | Pytest + GQL |
|---------|----------------|--------------|
| Tiempo ejecuciÃ³n | ~15s | **5.83s** âš¡ |
| Tests ejecutados | 20 | **33** |
| Setup/Teardown | Manual | AutomÃ¡tico |
| ValidaciÃ³n schema | No | SÃ­ |
| Reportes | Terminal | HTML + Terminal |
| ReutilizaciÃ³n cÃ³digo | Baja | Alta |

ğŸ“Š **Ver anÃ¡lisis completo en [COMPARACION.md](COMPARACION.md)**

## ğŸ“ Aprende MÃ¡s

- [Pytest Docs](https://docs.pytest.org/)
- [GQL Python](https://gql.readthedocs.io/)
- [Pytest Markers](https://docs.pytest.org/en/stable/mark.html)
- [Pytest Fixtures](https://docs.pytest.org/en/stable/fixture.html)

## âš™ï¸ ConfiguraciÃ³n Avanzada

### pytest.ini (opcional)
```ini
[pytest]
addopts = -v --tb=short
testpaths = .
python_files = test_*.py
python_classes = Test*
python_functions = test_*
markers =
    smoke: Quick smoke tests
    integration: Integration tests
    slow: Slow tests
```

## ğŸ†˜ SoluciÃ³n de Problemas

### Error: "No module named 'gql'"
```bash
pip install -r requirements.txt
```

### Error: "Schema validation failed"
AsegÃºrate de que el backend estÃ© corriendo en `http://localhost:8080`

### Tests muy lentos
Ejecuta solo tests smoke:
```bash
pytest -m smoke
```

## ğŸ“ Soporte

Si tienes problemas:
1. Verifica que el backend estÃ© corriendo
2. Revisa los logs con `pytest -vv`
3. Ejecuta un test individual para debugging
4. Consulta la documentaciÃ³n de Pytest
